{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b473ce12-1193-4b35-9731-ff770f081d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import networkx as nx\n",
    "import random\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import math\n",
    "import torch\n",
    "import wikipedia\n",
    "import IPython\n",
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "from networkx.drawing import nx_agraph\n",
    "import subprocess\n",
    "import base64\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1f307a-67c9-4d95-ba7d-cca932ca2d93",
   "metadata": {},
   "source": [
    "# SAMPLE INPUT TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fbf7e8a-17c6-4760-9ad1-b84b80fa0c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_input = \"\"\"Elena Cruz works at Google as a Software Engineer, possessing a robust skill set in Java, Python, and distributed systems. \n",
    "She has over five years of experience designing and developing scalable backend services that handle millions of requests daily. \n",
    "Elena began her journey at Google as a junior developer, quickly rising to mid-level and then senior engineer. She completed internal Google training on Cloud Architecture and participated in the company’s immersive Systems Design workshops. \n",
    "Elena also earned a Coursera specialization in Advanced Algorithms, further honing her ability to tackle large-scale technical challenges.\n",
    "\n",
    "Matthew Johnson is a Data Scientist at Google, specializing in Python, machine learning algorithms, and big data processing. \n",
    "With a background in statistics and a passion for data-driven decision-making, Matthew joined Google’s Data Analysis unit three years ago. \n",
    "He advanced from an associate analyst to a data scientist by mastering TensorFlow and working on cross-functional projects that integrated AI models into various Google services. \n",
    "He credits the Machine Learning Crash Course offered by Google and the “Women in Data Science at Google” training series for his development, \n",
    "and now focuses on predictive modeling and neural network research to drive product innovation.\n",
    "\n",
    "Sofia Kumar serves as a Product Manager at Google, leveraging user research, strategic roadmapping, and stakeholder collaboration. \n",
    "She began her career at Google as a marketing associate before transitioning into product operations and eventually product management. \n",
    "Sofia led cross-departmental teams in launching high-impact features for Google Workspace, refining her leadership style through the Google Project Management Training Program. \n",
    "She also completed Agile Methodologies training and, as a result of her accomplishments, is now aiming to become a Group Product Manager. \n",
    "Some texts only mention Sofia but they refer to the same person—Sofia Kumar.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490e3a2d-81ad-4dc5-b08e-f29e6c43bdc8",
   "metadata": {},
   "source": [
    "# LOAD MODEL AND TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08b66ff2-7e05-41bd-bb44-59a3ccf340fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "transformers_model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13c7c50-3324-4b22-a4fa-0877997f3c58",
   "metadata": {},
   "source": [
    "# TEXT NORMALIZATION & DUPLICATE CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c668f6e-af34-4e75-8d18-e98324b33904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Lowercases text, strips whitespace, optionally removes punctuation.\n",
    "    You can expand this function for more sophisticated normalization.\n",
    "    \"\"\"\n",
    "    text = text.strip().lower()\n",
    "    # Optional: remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def find_similar_entity(new_entity, existing_entities):\n",
    "    \"\"\"\n",
    "    Check if 'new_entity' is a substring or a superstring\n",
    "    of any existing entity (in a normalized sense).\n",
    "    If found, return the existing entity name to unify them.\n",
    "    Otherwise, return None.\n",
    "    \"\"\"\n",
    "    norm_new = normalize_text(new_entity)\n",
    "\n",
    "    for e in existing_entities:\n",
    "        norm_existing = normalize_text(e)\n",
    "        # If either is a substring of the other, consider them the same\n",
    "        if norm_new in norm_existing or norm_existing in norm_new:\n",
    "            return e\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8784d1-06c5-4d69-88e9-09511b670076",
   "metadata": {},
   "source": [
    "# HELPER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8b868cf-255c-4819-a7eb-b2f2918ea9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_model_output(model_text):\n",
    "    \"\"\"\n",
    "    Parse the REBEL model output to extract relations in\n",
    "    the form of (subject, relation, object).\n",
    "    \"\"\"\n",
    "    relation_list = []\n",
    "    subject_str, relation_str, object_str = '', '', ''\n",
    "    current_field = ''\n",
    "    \n",
    "    # Remove special tokens that we don't need\n",
    "    cleaned_output = (model_text\n",
    "                      .replace(\"<s>\", \"\")\n",
    "                      .replace(\"<pad>\", \"\")\n",
    "                      .replace(\"</s>\", \"\")\n",
    "                      .strip())\n",
    "    \n",
    "    for token in cleaned_output.split():\n",
    "        if token == \"<triplet>\":\n",
    "            current_field = 'SUBJECT'\n",
    "            if relation_str != '':\n",
    "                relation_list.append({\n",
    "                    'head': subject_str.strip(),\n",
    "                    'type': relation_str.strip(),\n",
    "                    'tail': object_str.strip()\n",
    "                })\n",
    "                relation_str = ''\n",
    "            subject_str = ''\n",
    "        \n",
    "        elif token == \"<subj>\":\n",
    "            current_field = 'OBJECT'\n",
    "            if relation_str != '':\n",
    "                relation_list.append({\n",
    "                    'head': subject_str.strip(),\n",
    "                    'type': relation_str.strip(),\n",
    "                    'tail': object_str.strip()\n",
    "                })\n",
    "            object_str = ''\n",
    "        \n",
    "        elif token == \"<obj>\":\n",
    "            current_field = 'RELATION'\n",
    "            relation_str = ''\n",
    "        \n",
    "        else:\n",
    "            if current_field == 'SUBJECT':\n",
    "                subject_str += ' ' + token\n",
    "            elif current_field == 'OBJECT':\n",
    "                object_str += ' ' + token\n",
    "            elif current_field == 'RELATION':\n",
    "                relation_str += ' ' + token\n",
    "    \n",
    "    # Catch any final leftover relation\n",
    "    if subject_str != '' and relation_str != '' and object_str != '':\n",
    "        relation_list.append({\n",
    "            'head': subject_str.strip(),\n",
    "            'type': relation_str.strip(),\n",
    "            'tail': object_str.strip()\n",
    "        })\n",
    "    \n",
    "    return relation_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb3de3-6fc3-47ad-8170-cd37320ad85b",
   "metadata": {},
   "source": [
    "# KNOWLEDGE BASE CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa2a5ea-31c1-4f1c-8aea-50c6baa5fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeBaseManager:\n",
    "    def __init__(self):\n",
    "        self.all_relations = []\n",
    "        self.known_entities = set()  # Keep track of all entity labels\n",
    "\n",
    "    def are_triples_same(self, triple_a, triple_b):\n",
    "        \"\"\"\n",
    "        Checks if two relations have the exact same triple:\n",
    "        subject (head), relation (type), object (tail).\n",
    "        \"\"\"\n",
    "        return (triple_a[\"head\"] == triple_b[\"head\"]\n",
    "                and triple_a[\"type\"] == triple_b[\"type\"]\n",
    "                and triple_a[\"tail\"] == triple_b[\"tail\"])\n",
    "\n",
    "    def is_relation_present(self, triple_data):\n",
    "        \"\"\"\n",
    "        Determine if a relation already exists in the knowledge base.\n",
    "        \"\"\"\n",
    "        for existing_triple in self.all_relations:\n",
    "            if self.are_triples_same(triple_data, existing_triple):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def combine_relation_data(self, new_triple):\n",
    "        \"\"\"\n",
    "        If a relation already exists, merge any extra metadata (like spans).\n",
    "        \"\"\"\n",
    "        for stored_triple in self.all_relations:\n",
    "            if self.are_triples_same(new_triple, stored_triple):\n",
    "                # Merge new spans that are not already present.\n",
    "                new_spans = [\n",
    "                    s for s in new_triple[\"meta\"][\"spans\"]\n",
    "                    if s not in stored_triple[\"meta\"][\"spans\"]\n",
    "                ]\n",
    "                stored_triple[\"meta\"][\"spans\"] += new_spans\n",
    "                return\n",
    "\n",
    "    def unify_entity_label(self, label):\n",
    "        \"\"\"\n",
    "        Attempt to unify an entity label with known entities \n",
    "        using approximate or substring matching. If found, return the known label.\n",
    "        Otherwise, add and return the new label.\n",
    "        \"\"\"\n",
    "        # Step 1: Check if there's a near match in known_entities\n",
    "        matched_label = find_similar_entity(label, self.known_entities)\n",
    "        if matched_label is not None:\n",
    "            # Return the existing label\n",
    "            return matched_label\n",
    "        else:\n",
    "            # If not found, add label to known_entities\n",
    "            self.known_entities.add(label)\n",
    "            return label\n",
    "\n",
    "    def insert_relation(self, triple_data):\n",
    "        \"\"\"\n",
    "        Adds a relation to the knowledge base if it does not exist.\n",
    "        If it exists, merges the new metadata.\n",
    "        \"\"\"\n",
    "        # Normalize HEAD\n",
    "        unified_head = self.unify_entity_label(triple_data[\"head\"])\n",
    "        # Normalize TAIL\n",
    "        unified_tail = self.unify_entity_label(triple_data[\"tail\"])\n",
    "\n",
    "        triple_data[\"head\"] = unified_head\n",
    "        triple_data[\"tail\"] = unified_tail\n",
    "\n",
    "        if not self.is_relation_present(triple_data):\n",
    "            self.all_relations.append(triple_data)\n",
    "        else:\n",
    "            self.combine_relation_data(triple_data)\n",
    "\n",
    "    def display_relations(self):\n",
    "        \"\"\"\n",
    "        Prints all relations in the knowledge base.\n",
    "        \"\"\"\n",
    "        print(\"Knowledge Base Relations:\")\n",
    "        for idx, rel_item in enumerate(self.all_relations, start=1):\n",
    "            print(f\"{idx:02d}. {rel_item}\")\n",
    "\n",
    "    def get_relations(self):\n",
    "        \"\"\"\n",
    "        Returns the entire list of relations stored.\n",
    "        \"\"\"\n",
    "        return self.all_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e444c-64d7-42bd-9313-3b743a9f3dc3",
   "metadata": {},
   "source": [
    "# NEW: SENTENCE-BY-SENTENCE KB CONSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "136e30f8-2061-45dd-a311-ac17947aa4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kb_from_sentences(input_text, show_verbose=False):\n",
    "    \"\"\"\n",
    "    Use spaCy (or another approach) to split the text into sentences.\n",
    "    Then process each sentence separately to ensure we capture all\n",
    "    relations, and combine the results into one KnowledgeBaseManager.\n",
    "    \"\"\"\n",
    "    # Load spaCy's small English model (ensure installed: python -m spacy download en_core_web_sm)\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    doc = nlp(input_text)\n",
    "    sentence_list = [sent.text.strip() for sent in doc.sents]\n",
    "    \n",
    "    if show_verbose:\n",
    "        print(\"Splitting text into sentences:\")\n",
    "        for s in sentence_list:\n",
    "            print(f\"- {s}\")\n",
    "\n",
    "    # Create a single KB manager to combine all sentence-level KBs\n",
    "    combined_kb = KnowledgeBaseManager()\n",
    "\n",
    "    for sent_index, sentence_text in enumerate(sentence_list):\n",
    "        # Build a temporary KB from one sentence\n",
    "        temp_kb = build_knowledgebase_from_text(\n",
    "            sentence_text,\n",
    "            span_limit=64,           # smaller spans since sentences are short\n",
    "            show_verbose=show_verbose\n",
    "        )\n",
    "\n",
    "        # Merge the temporary KB's relations into the combined KB\n",
    "        for r in temp_kb.get_relations():\n",
    "            combined_kb.insert_relation(r)\n",
    "    \n",
    "    return combined_kb\n",
    "\n",
    "def build_knowledgebase_from_text(input_text, span_limit=128, show_verbose=False):\n",
    "    \"\"\"\n",
    "    Splits input text (or a single sentence) into overlapping spans, feeds each span\n",
    "    into the REBEL model to extract relations, and compiles them\n",
    "    into a KnowledgeBaseManager object.\n",
    "    \"\"\"\n",
    "    # Tokenize the entire text\n",
    "    tokenized_input = transformers_tokenizer([input_text], return_tensors=\"pt\")\n",
    "\n",
    "    # Calculate total tokens\n",
    "    total_tokens = len(tokenized_input[\"input_ids\"][0])\n",
    "    if show_verbose:\n",
    "        print(f\"Total tokens in input: {total_tokens}\")\n",
    "\n",
    "    # Calculate how many spans are needed\n",
    "    total_spans = math.ceil(total_tokens / span_limit)\n",
    "    if show_verbose:\n",
    "        print(f\"Total spans needed: {total_spans}\")\n",
    "\n",
    "    # Overlap calculation for the spans\n",
    "    overlap_count = math.ceil(\n",
    "        (total_spans * span_limit - total_tokens) /\n",
    "        max(total_spans - 1, 1)\n",
    "    )\n",
    "\n",
    "    # Determine span boundaries\n",
    "    boundary_indices = []\n",
    "    start_pos = 0\n",
    "    for i in range(total_spans):\n",
    "        boundary_indices.append([\n",
    "            start_pos + span_limit * i,\n",
    "            start_pos + span_limit * (i + 1)\n",
    "        ])\n",
    "        start_pos -= overlap_count\n",
    "\n",
    "    if show_verbose:\n",
    "        print(\"Span boundaries determined:\")\n",
    "        for boundary in boundary_indices:\n",
    "            print(boundary)\n",
    "\n",
    "    # Split the tokens/mask into spans\n",
    "    chunked_input_ids = [\n",
    "        tokenized_input[\"input_ids\"][0][b[0]:b[1]]\n",
    "        for b in boundary_indices\n",
    "    ]\n",
    "    chunked_attention_masks = [\n",
    "        tokenized_input[\"attention_mask\"][0][b[0]:b[1]]\n",
    "        for b in boundary_indices\n",
    "    ]\n",
    "\n",
    "    # Create an expanded input dictionary\n",
    "    stacked_inputs = {\n",
    "        \"input_ids\": torch.stack(chunked_input_ids),\n",
    "        \"attention_mask\": torch.stack(chunked_attention_masks)\n",
    "    }\n",
    "\n",
    "    # Generation config\n",
    "    generation_params = {\n",
    "        \"max_length\": 256,\n",
    "        \"length_penalty\": 0,\n",
    "        \"num_beams\": 3,\n",
    "        \"num_return_sequences\": 3\n",
    "    }\n",
    "\n",
    "    # Generate outputs for each span\n",
    "    model_generated_outputs = transformers_model.generate(\n",
    "        **stacked_inputs,\n",
    "        **generation_params\n",
    "    )\n",
    "\n",
    "    # Decode the outputs\n",
    "    decoded_model_texts = transformers_tokenizer.batch_decode(\n",
    "        model_generated_outputs,\n",
    "        skip_special_tokens=False\n",
    "    )\n",
    "\n",
    "    # Create a knowledge base instance\n",
    "    kb_instance = KnowledgeBaseManager()\n",
    "\n",
    "    for idx, decoded_text in enumerate(decoded_model_texts):\n",
    "        # Determine which span this result came from\n",
    "        current_span_index = idx // generation_params[\"num_return_sequences\"]\n",
    "        extracted_relations = interpret_model_output(decoded_text)\n",
    "\n",
    "        for rel_obj in extracted_relations:\n",
    "            rel_obj[\"meta\"] = {\n",
    "                \"spans\": [boundary_indices[current_span_index]]\n",
    "            }\n",
    "            kb_instance.insert_relation(rel_obj)\n",
    "    \n",
    "    return kb_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274ad093-eeb0-41cb-a1d3-28471151ed38",
   "metadata": {},
   "source": [
    "# EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3427a553-8dba-4139-8c03-aa0219c32d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Base Relations:\n",
      "01. {'head': 'Elena Cruz', 'type': 'employer', 'tail': 'Google', 'meta': {'spans': [[0, 64]]}}\n",
      "02. {'head': 'Elena Cruz', 'type': 'occupation', 'tail': 'Software Engineer', 'meta': {'spans': [[0, 64]]}}\n",
      "03. {'head': 'backend', 'type': 'subclass of', 'tail': 'services', 'meta': {'spans': [[0, 64]]}}\n",
      "04. {'head': 'services', 'type': 'subclass of', 'tail': 'services', 'meta': {'spans': [[0, 64]]}}\n",
      "05. {'head': 'services', 'type': 'subclass of', 'tail': 'scalable', 'meta': {'spans': [[0, 64]]}}\n",
      "06. {'head': 'junior developer', 'type': 'followed by', 'tail': 'senior engineer', 'meta': {'spans': [[0, 64]]}}\n",
      "07. {'head': 'senior engineer', 'type': 'follows', 'tail': 'junior developer', 'meta': {'spans': [[0, 64]]}}\n",
      "08. {'head': 'Cloud Architecture', 'type': 'subclass of', 'tail': 'Systems Design', 'meta': {'spans': [[0, 64]]}}\n",
      "09. {'head': 'Cloud Architecture', 'type': 'part of', 'tail': 'Systems Design', 'meta': {'spans': [[0, 64]]}}\n",
      "10. {'head': 'Systems Design', 'type': 'part of', 'tail': 'Cloud Architecture', 'meta': {'spans': [[0, 64]]}}\n",
      "11. {'head': 'Coursera', 'type': 'field of work', 'tail': 'Advanced Algorithms', 'meta': {'spans': [[0, 64]]}}\n",
      "12. {'head': 'Advanced Algorithms', 'type': 'part of', 'tail': 'Coursera', 'meta': {'spans': [[0, 64]]}}\n",
      "13. {'head': 'Coursera', 'type': 'product or material produced', 'tail': 'Advanced Algorithms', 'meta': {'spans': [[0, 64]]}}\n",
      "14. {'head': 'Matthew Johnson', 'type': 'occupation', 'tail': 'Data Scientist', 'meta': {'spans': [[0, 64]]}}\n",
      "15. {'head': 'Matthew Johnson', 'type': 'employer', 'tail': 'Google', 'meta': {'spans': [[0, 64]]}}\n",
      "16. {'head': 'Data Analysis', 'type': 'part of', 'tail': 'statistics', 'meta': {'spans': [[0, 64]]}}\n",
      "17. {'head': 'Data Analysis', 'type': 'facet of', 'tail': 'statistics', 'meta': {'spans': [[0, 64]]}}\n",
      "18. {'head': 'Data Analysis', 'type': 'subclass of', 'tail': 'statistics', 'meta': {'spans': [[0, 64]]}}\n",
      "19. {'head': 'TensorFlow', 'type': 'developer', 'tail': 'Google', 'meta': {'spans': [[0, 64]]}}\n",
      "20. {'head': 'TensorFlow', 'type': 'use', 'tail': 'AI', 'meta': {'spans': [[0, 64]]}}\n",
      "21. {'head': 'Machine Learning Crash Course', 'type': 'developer', 'tail': 'Google', 'meta': {'spans': [[0, 64]]}}\n",
      "22. {'head': 'Machine Learning Crash Course', 'type': 'part of', 'tail': 'Google', 'meta': {'spans': [[0, 64]]}}\n",
      "23. {'head': 'Machine Learning Crash Course', 'type': 'publisher', 'tail': 'Google', 'meta': {'spans': [[0, 64]]}}\n",
      "24. {'head': 'Sofia Kumar', 'type': 'employer', 'tail': 'Google', 'meta': {'spans': [[0, 64]]}}\n",
      "25. {'head': 'Sofia Kumar', 'type': 'occupation', 'tail': 'Product Manager', 'meta': {'spans': [[0, 64]]}}\n",
      "26. {'head': 'product management', 'type': 'facet of', 'tail': 'product operations', 'meta': {'spans': [[0, 64]]}}\n",
      "27. {'head': 'product management', 'type': 'part of', 'tail': 'product operations', 'meta': {'spans': [[0, 64]]}}\n",
      "28. {'head': 'product operations', 'type': 'part of', 'tail': 'product management', 'meta': {'spans': [[0, 64]]}}\n",
      "29. {'head': 'Google', 'type': 'developer', 'tail': 'Google', 'meta': {'spans': [[0, 64]]}}\n",
      "30. {'head': 'Google', 'type': 'developer', 'tail': 'AI', 'meta': {'spans': [[0, 64]]}}\n",
      "31. {'head': 'Agile', 'type': 'instance of', 'tail': 'Methodologies', 'meta': {'spans': [[0, 64]]}}\n",
      "32. {'head': 'Agile', 'type': 'used by', 'tail': 'Product Manager', 'meta': {'spans': [[0, 64]]}}\n",
      "33. {'head': 'Agile', 'type': 'practiced by', 'tail': 'Product Manager', 'meta': {'spans': [[0, 64]]}}\n",
      "34. {'head': 'Product Manager', 'type': 'field of this occupation', 'tail': 'Agile', 'meta': {'spans': [[0, 64]]}}\n",
      "35. {'head': 'Sofia Kumar', 'type': 'said to be the same as', 'tail': 'Sofia Kumar', 'meta': {'spans': [[0, 64]]}}\n",
      "36. {'head': 'Sofia Kumar', 'type': 'named after', 'tail': 'Sofia Kumar', 'meta': {'spans': [[0, 64]]}}\n"
     ]
    }
   ],
   "source": [
    "# Instead of passing the entire multi-sentence text directly,\n",
    "# we now split it by sentences and parse each sentence.\n",
    "final_kb = build_kb_from_sentences(content_input, show_verbose=False)\n",
    "final_kb.display_relations()\n",
    "\n",
    "# Build a directed graph from the combined relations\n",
    "graph_directed = nx.DiGraph()\n",
    "found_relations = final_kb.get_relations()\n",
    "\n",
    "for relation_info in found_relations:\n",
    "    head_node = relation_info[\"head\"]\n",
    "    tail_node = relation_info[\"tail\"]\n",
    "    edge_label = relation_info[\"type\"]\n",
    "    \n",
    "    graph_directed.add_node(head_node)\n",
    "    graph_directed.add_node(tail_node)\n",
    "    graph_directed.add_edge(head_node, tail_node, label=edge_label)\n",
    "\n",
    "# Write to DOT file\n",
    "nx_agraph.write_dot(graph_directed, \"kg.dot\")\n",
    "\n",
    "# Convert DOT to PNG using graphviz\n",
    "subprocess.run([\"dot\", \"-Tpng\", \"kg.dot\", \"-o\", \"kg.png\"])\n",
    "\n",
    "# Encode the PNG in Base64\n",
    "with open(\"kg.png\", \"rb\") as image_file:\n",
    "    base64_image_str = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Generate an HTML page with the embedded image\n",
    "html_document = f\"\"\"\\\n",
    "<html>\n",
    "<head>\n",
    "  <title>Generated Knowledge Graph</title>\n",
    "</head>\n",
    "<body>\n",
    "  <h1>Knowledge Graph</h1>\n",
    "  <!-- Base64-embedded PNG -->\n",
    "  <img src=\"data:image/png;base64,{base64_image_str}\" alt=\"Knowledge Graph\" />\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "with open(\"generated_knowledge_graph.html\", \"w\") as html_file:\n",
    "    html_file.write(html_document)\n",
    "\n",
    "# print(\"HTML file with the embedded knowledge-graph image saved as 'generated_knowledge_graph'\").html'\")\r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f43f248-dcf2-4f1b-9953-d189366c2863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
